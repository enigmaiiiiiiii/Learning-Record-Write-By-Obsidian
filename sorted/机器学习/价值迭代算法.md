# 价值迭代算法

## Sarsa

- 命名的由来：$s \rightarrow a \rightarrow r \rightarrow s \rightarrow a$
- 通过[[贝尔曼方程]]得到样本和预测的误差

动作价值的更新公式: 

$$
Q(s_t,a_t)=
Q(s_t,a_t)+\eta \cdot (
\underbrace {
  R_{t+1}+\gamma 
  \overbrace {
  Q(s_{t+1}, a_{t+1})
  }^\text{
    sarsa与Q区别的地方
  }
  -Q(s_t,a_t)
}_\text{
bellman function
})
$$
$\eta$： [[学习率]]
$\gamma$:  奖励折扣率

## Q学习


$$
Q(s_t,a_t)=
Q(s_t,a_t)+\eta \cdot (
\underbrace {
  R_{t+1}+\gamma 
  \overbrace {
  Q(s_{t+1}, a)
  }^\text{
    sarsa与Q区别的地方
  }
  -Q(s_t,a_t)
}_\text{
bellman function
})
$$

- Q学习的更新不依赖于下一步动作