# 贝尔曼方程

- 贝尔曼方程前提条件，学习对象必须是[马尔可夫决策过程](markov-process.md) [^Markov]

##  符号
- 状态: $s_t$
- 动作: $a$
- 奖励：$R,r$
- 衰减：$\gamma, \gamma \in [0,1]$
- 策略：$\pi$
- 动作空间: A


## 贝尔曼最优方程

- 最优状态价值

$$
V^*(s)=\max \limits_{a} \sum \limits_{s',r} p(s',r|s,a)[r+\gamma V^*(s')]
$$

状态价值等于最大动作价值

> $V^*(s)=\max\limits_{a}q^*(s,a)$

- 最优动作价值

$$
q^*(s,a)=\sum\limits_{s',r} p(s',a|s,a)[r+r\max \limits_{a'} q^*(s',a')]
$$


## 贝尔曼期望方程

- 状态期望

$$
V^\pi(s)=\sum\limits_{a\in A}\pi(a|s)\sum \limits_{s',r} P(s',r|s,a)[r+\gamma V^\pi(s')]
$$

- 动作期望

$$
q^\pi(s,a)=\sum\limits_{s',r} P(s',r|s,a)[r+r\sum\limits_{a'}\pi(a'|s')q^\pi(s',a')]
$$



